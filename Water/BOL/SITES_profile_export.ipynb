{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories that may be needed\n",
    "home_dir = os.getcwd()\n",
    "in_dir = '../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataimport\n",
    "# Note: the file import is currently custom and needs adjustment with new data\n",
    "df = pd.read_csv(in_dir + 'BOL/exp/19350701_20250122_BOL_COMPLETE.csv', \n",
    "                 parse_dates = True, \n",
    "                 low_memory=False,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SITES uses different names for the parameter headers, which need to be adjusted before uploading to the portal. \n",
    "This dict is meant to 'translate' this differences. \n",
    "\"\"\"\n",
    "\n",
    "col_dict_SITES = {\n",
    "    'Water depth [m]' : 'D_Water',\n",
    "    'Water temperature [°C]' : 'TW',\n",
    "    'EC [µS/cm]' : 'EC',\n",
    "    'SCOND [µS/cm]' : 'SCOND',\n",
    "    'pH' : 'pH', \n",
    "    'ORP [mV]' : 'ORP',\n",
    "    'Turbidity [FNU]' : 'TURBF',\n",
    "    'Chlorophyll [µg/L]' : 'CHLF',\n",
    "    'O2SAT [%]' : 'O2SAT',\n",
    "    'O2 [mg/L]' : 'O2',\n",
    "    'fDOM [QSU]' : 'FDOM'\n",
    "}\n",
    "\n",
    "# List profile locations for SITES  \n",
    "sites_profil = ['Bol NW', 'Bol NE', 'Bol E', 'Bol S', 'Bol W']\n",
    "\n",
    "# Set last DN uploaded on SITES DataPortal\n",
    "\"\"\" \n",
    "The DN of each location needs to be adjusted to allow consistency with the SITES dataportal. Currently the DN is not consistent due to previous handcleaning of the data. \n",
    "The following dictionary holds the information about the last DN uploaded for each location. \n",
    "\"\"\"\n",
    "\n",
    "DN_portal = {'Bol NW' : 14, \n",
    "             'Bol NE' : 16, \n",
    "             'Bol E' : 16, \n",
    "             'Bol S' : 15, \n",
    "             'Bol W': 17\n",
    "             }\n",
    "\n",
    "# List of needed columns \n",
    "cols_of_interest = ['Time',\n",
    "                    'Date',\n",
    "                    'Site',\n",
    "                    'DN',\n",
    "                    'Water depth [m]',\n",
    "                    'Water temperature [°C]',\n",
    "                    'EC [µS/cm]',\n",
    "                    'SCOND [µS/cm]',\n",
    "                    'pH', \n",
    "                    'ORP [mV]',\n",
    "                    'Turbidity [FNU]',\n",
    "                    'Chlorophyll [µg/L]',\n",
    "                    'O2SAT [%]',\n",
    "                    'O2 [mg/L]',\n",
    "                    'fDOM [QSU]'\n",
    "                    ]\n",
    "\n",
    "# Define sorting of columns to be in line with the header file found in 'SITES_headers'\n",
    "sorting = ['TIMESTAMP', 'DN', 'D_Water', \n",
    "           'TW', 'EC', 'SCOND', 'pH',\n",
    "           'ORP', 'TURBF', 'CHLF', 'O2SAT', \n",
    "           'O2', 'FDOM'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data\n",
    "tmp = df[(df['Site'].isin(sites_profil)) & (df['Datasource']=='EXO_KOR') & (df['Date']>'2023-10-05')]\n",
    "tmp = tmp[cols_of_interest]\n",
    "\n",
    "# export data per station \n",
    "for site in sites_profil:\n",
    "    # Select data from input dataset\n",
    "    out = tmp[tmp['Site']==site]\n",
    "    \n",
    "    # Concatenate date and time into single column \n",
    "    out['TIMESTAMP'] = pd.to_datetime(tmp['Date'] + ' ' + tmp['Time'])\n",
    "    out.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "    \n",
    "    # Rename columns to fit SITES\n",
    "    out = out.rename(columns=col_dict_SITES)\n",
    "\n",
    "    # Sort columns\n",
    "    out = out.loc[:, sorting]\n",
    "\n",
    "    # adjust DN\n",
    "    out['DN'] = out['DN'] - (out['DN'].iloc[0] - 1)\n",
    "    out['DN'] = (out['DN'] + DN_portal[site]).astype(int)\n",
    "\n",
    "    # read header\n",
    "    header = pd.read_csv(f\"SITES_headers/head-PROF_{site}.csv\",\n",
    "                          sep=';', header=None,\n",
    "                          skipfooter=1,\n",
    "                          )\n",
    "    \n",
    "    ## adjust time period in header file\n",
    "    start_date = out['TIMESTAMP'].iloc[0].date()\n",
    "    end_date = out['TIMESTAMP'].iloc[-1].date()\n",
    "    TimePeriod = f\"{start_date} - {end_date}\"\n",
    "    header.iloc[2, 0] = header.iloc[2, 0].replace('yyyy-mm-dd - yyyy-mm-dd', TimePeriod)\n",
    "\n",
    "    # remove seconds in timestamp\n",
    "    out['TIMESTAMP'] = out['TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    # temporary store data \n",
    "    out.to_csv('tmp/data.csv', index=None)\n",
    "    header.to_csv('tmp/header.csv', index=None, header=None)\n",
    "\n",
    "    # combine header and data\n",
    "    ## Reading header from tmp\n",
    "    with open('tmp/header.csv') as fp:\n",
    "        data = fp.read().replace('\"',\"\")\n",
    "\n",
    "    ## Reading data from tmp\n",
    "    with open('tmp/data.csv') as fp:\n",
    "        data2 = fp.read()\n",
    "\n",
    "    ## merging header and data\n",
    "    data += data2\n",
    "\n",
    "    # write output file\n",
    "    start_date_iso = start_date.isoformat().replace('-','')\n",
    "    end_date_iso = end_date.isoformat().replace('-','')\n",
    "    with open (f\"../exp/SITES_SONDE-PROF_BOL_BS-{site[4:]}_{start_date_iso}-{end_date_iso}_L2_irregular.csv\", 'w') as fp:\n",
    "        fp.write(data)\n",
    "\n",
    "    # delete tmp output files\n",
    "    os.remove('tmp/header.csv')\n",
    "    os.remove('tmp/data.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
